#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å®Œå–„æ¡ˆä¾‹å…ƒæ•°æ®è„šæœ¬
ä»æ¡ˆä¾‹READMEæ–‡ä»¶ä¸­æå–è¯¦ç»†ä¿¡æ¯å¹¶æ›´æ–°æ•°æ®åº“
"""

import sys
import json
import re
from pathlib import Path
from typing import Dict, List, Optional

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from services.textbook.database import SessionLocal
from services.textbook.models import ChapterCaseMapping

# æ¡ˆä¾‹ç´¢å¼•æ–‡ä»¶
CASES_INDEX_FILE = Path(__file__).parent.parent / "cases_index.json"
BOOKS_BASE_DIR = Path(__file__).parent.parent.parent.parent  # CHS-Books root


def load_cases_index():
    """åŠ è½½æ¡ˆä¾‹ç´¢å¼•"""
    if not CASES_INDEX_FILE.exists():
        print(f"[ERROR] æ¡ˆä¾‹ç´¢å¼•æ–‡ä»¶ä¸å­˜åœ¨: {CASES_INDEX_FILE}")
        return {"books": []}
    
    with open(CASES_INDEX_FILE, 'r', encoding='utf-8') as f:
        return json.load(f)


def extract_metadata_from_readme(readme_path: Path) -> Dict:
    """ä»READMEæ–‡ä»¶ä¸­æå–å…ƒæ•°æ®"""
    if not readme_path.exists():
        return {}
    
    try:
        content = readme_path.read_text(encoding='utf-8')
    except Exception as e:
        print(f"[WARN] è¯»å–READMEå¤±è´¥ {readme_path}: {e}")
        return {}
    
    metadata = {
        'difficulty': None,
        'estimated_time': None,
        'learning_objectives': [],
        'key_concepts': [],
        'tags': [],
        'prerequisites': [],
        'control_methods': []
    }
    
    # æå–éš¾åº¦ç­‰çº§
    difficulty_patterns = [
        r'éš¾åº¦[ï¼š:]\s*([â­â˜…]+)',
        r'éš¾åº¦çº§åˆ«[ï¼š:]\s*(\w+)',
        r'\*\*éš¾åº¦\*\*[ï¼š:]\s*([â­â˜…]+)'
    ]
    for pattern in difficulty_patterns:
        match = re.search(pattern, content)
        if match:
            difficulty_str = match.group(1)
            star_count = len([c for c in difficulty_str if c in 'â­â˜…'])
            if star_count == 1:
                metadata['difficulty'] = 'åˆçº§'
            elif star_count == 2:
                metadata['difficulty'] = 'åˆçº§'
            elif star_count == 3:
                metadata['difficulty'] = 'ä¸­çº§'
            elif star_count == 4:
                metadata['difficulty'] = 'é«˜çº§'
            else:
                metadata['difficulty'] = 'ä¸“å®¶'
            break
    
    # æå–é¢„è®¡æ—¶é—´
    time_patterns = [
        r'é¢„è®¡æ—¶é—´[ï¼š:]\s*(\d+)\s*åˆ†é’Ÿ',
        r'å­¦ä¹ æ—¶é•¿[ï¼š:]\s*(\d+)\s*åˆ†é’Ÿ',
        r'æ—¶é—´[ï¼š:]\s*(\d+)\s*min'
    ]
    for pattern in time_patterns:
        match = re.search(pattern, content)
        if match:
            metadata['estimated_time'] = int(match.group(1))
            break
    
    # æå–å­¦ä¹ ç›®æ ‡
    objectives_section = re.search(r'##\s*å­¦ä¹ ç›®æ ‡(.*?)(?=##|$)', content, re.DOTALL)
    if objectives_section:
        objectives_text = objectives_section.group(1)
        # æå–åˆ—è¡¨é¡¹
        objectives = re.findall(r'[-*]\s*(.+)', objectives_text)
        metadata['learning_objectives'] = [obj.strip() for obj in objectives if obj.strip()]
    
    # æå–å…³é”®æ¦‚å¿µ/æ ¸å¿ƒå†…å®¹
    concepts_patterns = [
        r'##\s*å…³é”®æ¦‚å¿µ(.*?)(?=##|$)',
        r'##\s*æ ¸å¿ƒå†…å®¹(.*?)(?=##|$)',
        r'##\s*æ¶‰åŠç†è®º(.*?)(?=##|$)'
    ]
    for pattern in concepts_patterns:
        concepts_section = re.search(pattern, content, re.DOTALL)
        if concepts_section:
            concepts_text = concepts_section.group(1)
            concepts = re.findall(r'[-*]\s*(.+)', concepts_text)
            metadata['key_concepts'].extend([c.strip() for c in concepts if c.strip()])
    
    # æå–æ§åˆ¶æ–¹æ³•
    control_keywords = {
        'PID': 'PIDæ§åˆ¶',
        'PIæ§åˆ¶': 'PIæ§åˆ¶',
        'PDæ§åˆ¶': 'PDæ§åˆ¶',
        'å¼€å…³æ§åˆ¶': 'å¼€å…³æ§åˆ¶',
        'MPC': 'æ¨¡å‹é¢„æµ‹æ§åˆ¶',
        'æ¨¡å‹é¢„æµ‹æ§åˆ¶': 'æ¨¡å‹é¢„æµ‹æ§åˆ¶',
        'LQR': 'LQRæœ€ä¼˜æ§åˆ¶',
        'çŠ¶æ€åé¦ˆ': 'çŠ¶æ€åé¦ˆ',
        'ä¸²çº§æ§åˆ¶': 'ä¸²çº§æ§åˆ¶',
        'å‰é¦ˆæ§åˆ¶': 'å‰é¦ˆæ§åˆ¶',
        'è‡ªé€‚åº”æ§åˆ¶': 'è‡ªé€‚åº”æ§åˆ¶',
        'æ»‘æ¨¡æ§åˆ¶': 'æ»‘æ¨¡æ§åˆ¶',
        'æ¨¡ç³Šæ§åˆ¶': 'æ¨¡ç³Šæ§åˆ¶',
        'ç¥ç»ç½‘ç»œ': 'ç¥ç»ç½‘ç»œæ§åˆ¶',
        'å¼ºåŒ–å­¦ä¹ ': 'å¼ºåŒ–å­¦ä¹ æ§åˆ¶'
    }
    
    for keyword, method in control_keywords.items():
        if keyword in content:
            if method not in metadata['control_methods']:
                metadata['control_methods'].append(method)
    
    # æå–æ ‡ç­¾
    tags_section = re.search(r'æ ‡ç­¾[ï¼š:]\s*(.+)', content)
    if tags_section:
        tags_text = tags_section.group(1)
        tags = re.findall(r'`([^`]+)`', tags_text)
        metadata['tags'] = tags
    else:
        # ä»æ§åˆ¶æ–¹æ³•ç”Ÿæˆæ ‡ç­¾
        metadata['tags'] = metadata['control_methods'][:5]
    
    return metadata


def save_enhanced_metadata():
    """ä¿å­˜å¢å¼ºçš„æ¡ˆä¾‹å…ƒæ•°æ®åˆ°JSONæ–‡ä»¶"""
    cases_index = load_cases_index()
    
    enhanced_cases = []
    
    for book_data in cases_index.get("books", []):
        book_slug = book_data.get("slug", "")
        
        for case in book_data.get("cases", []):
            case_id = case.get("id")
            case_path = BOOKS_BASE_DIR / "books" / book_slug / "code" / "examples" / case_id
            readme_path = case_path / "README.md"
            
            # æå–å…ƒæ•°æ®
            metadata = extract_metadata_from_readme(readme_path)
            
            # åˆå¹¶å…ƒæ•°æ®
            enhanced_case = {
                **case,
                "book_slug": book_slug,
                **metadata
            }
            
            enhanced_cases.append(enhanced_case)
    
    # ä¿å­˜åˆ°JSONæ–‡ä»¶
    output_file = Path(__file__).parent.parent / "enhanced_cases_metadata.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump({
            "total_cases": len(enhanced_cases),
            "cases": enhanced_cases
        }, f, ensure_ascii=False, indent=2)
    
    print(f"[OK] å·²ä¿å­˜å¢å¼ºå…ƒæ•°æ®åˆ°: {output_file}")
    return enhanced_cases


def update_chapter_case_mappings(enhanced_cases: List[Dict]):
    """æ›´æ–°ç« èŠ‚-æ¡ˆä¾‹å…³è”çš„æè¿°ä¿¡æ¯"""
    db = SessionLocal()
    
    try:
        # åˆ›å»ºæ¡ˆä¾‹å…ƒæ•°æ®æ˜ å°„
        case_metadata_map = {case['id']: case for case in enhanced_cases}
        
        # è·å–æ‰€æœ‰å…³è”è®°å½•
        mappings = db.query(ChapterCaseMapping).all()
        
        updated_count = 0
        
        for mapping in mappings:
            case_id = mapping.case_id
            
            if case_id in case_metadata_map:
                case_meta = case_metadata_map[case_id]
                
                # æ›´æ–°æè¿°
                description_parts = []
                
                if case_meta.get('difficulty'):
                    description_parts.append(f"éš¾åº¦: {case_meta['difficulty']}")
                
                if case_meta.get('control_methods'):
                    methods = ', '.join(case_meta['control_methods'][:3])
                    description_parts.append(f"æ§åˆ¶æ–¹æ³•: {methods}")
                
                if case_meta.get('estimated_time'):
                    description_parts.append(f"æ—¶é•¿: {case_meta['estimated_time']}åˆ†é’Ÿ")
                
                if description_parts:
                    mapping.description = '; '.join(description_parts)
                    updated_count += 1
        
        db.commit()
        print(f"[OK] æ›´æ–°äº† {updated_count} æ¡å…³è”è®°å½•çš„æè¿°ä¿¡æ¯")
        
    except Exception as e:
        print(f"[ERROR] æ›´æ–°å¤±è´¥: {e}")
        db.rollback()
    finally:
        db.close()


def generate_statistics(enhanced_cases: List[Dict]):
    """ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯"""
    print("\n" + "="*60)
    print("  æ¡ˆä¾‹å…ƒæ•°æ®ç»Ÿè®¡")
    print("="*60 + "\n")
    
    # éš¾åº¦åˆ†å¸ƒ
    difficulty_count = {}
    for case in enhanced_cases:
        diff = case.get('difficulty', 'æœªçŸ¥')
        difficulty_count[diff] = difficulty_count.get(diff, 0) + 1
    
    print("ğŸ“Š éš¾åº¦åˆ†å¸ƒ:")
    for diff, count in sorted(difficulty_count.items(), key=lambda x: (x[0] is None, x[0])):
        print(f"  {diff if diff else 'æœªçŸ¥'}: {count} ä¸ªæ¡ˆä¾‹")
    
    # æ§åˆ¶æ–¹æ³•ç»Ÿè®¡
    method_count = {}
    for case in enhanced_cases:
        for method in case.get('control_methods', []):
            method_count[method] = method_count.get(method, 0) + 1
    
    print("\nğŸ¯ æ§åˆ¶æ–¹æ³•ç»Ÿè®¡ (Top 10):")
    sorted_methods = sorted(method_count.items(), key=lambda x: x[1], reverse=True)
    for method, count in sorted_methods[:10]:
        print(f"  {method}: {count} ä¸ªæ¡ˆä¾‹")
    
    # å­¦ä¹ æ—¶é—´ç»Ÿè®¡
    time_cases = [c for c in enhanced_cases if c.get('estimated_time')]
    if time_cases:
        avg_time = sum(c['estimated_time'] for c in time_cases) / len(time_cases)
        min_time = min(c['estimated_time'] for c in time_cases)
        max_time = max(c['estimated_time'] for c in time_cases)
        
        print(f"\nâ±ï¸  å­¦ä¹ æ—¶é—´ç»Ÿè®¡:")
        print(f"  å¹³å‡æ—¶é•¿: {avg_time:.0f} åˆ†é’Ÿ")
        print(f"  æœ€çŸ­: {min_time} åˆ†é’Ÿ")
        print(f"  æœ€é•¿: {max_time} åˆ†é’Ÿ")
    
    # æœ‰å­¦ä¹ ç›®æ ‡çš„æ¡ˆä¾‹æ•°
    with_objectives = len([c for c in enhanced_cases if c.get('learning_objectives')])
    print(f"\nğŸ“ æœ‰å­¦ä¹ ç›®æ ‡çš„æ¡ˆä¾‹: {with_objectives}/{len(enhanced_cases)}")
    
    # æœ‰å…³é”®æ¦‚å¿µçš„æ¡ˆä¾‹æ•°
    with_concepts = len([c for c in enhanced_cases if c.get('key_concepts')])
    print(f"ğŸ’¡ æœ‰å…³é”®æ¦‚å¿µçš„æ¡ˆä¾‹: {with_concepts}/{len(enhanced_cases)}")


def main():
    print("\n" + "="*60)
    print("  å®Œå–„æ¡ˆä¾‹å…ƒæ•°æ®")
    print("="*60 + "\n")
    
    print("[STEP 1] ä»READMEæ–‡ä»¶æå–å…ƒæ•°æ®...")
    enhanced_cases = save_enhanced_metadata()
    
    print(f"\n[STEP 2] å¤„ç†äº† {len(enhanced_cases)} ä¸ªæ¡ˆä¾‹")
    
    print("\n[STEP 3] æ›´æ–°ç« èŠ‚-æ¡ˆä¾‹å…³è”æè¿°...")
    update_chapter_case_mappings(enhanced_cases)
    
    print("\n[STEP 4] ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯...")
    generate_statistics(enhanced_cases)
    
    print("\n" + "="*60)
    print("  å…ƒæ•°æ®å®Œå–„å®Œæˆ")
    print("="*60 + "\n")


if __name__ == "__main__":
    import sys
    import io
    
    # è®¾ç½®UTF-8è¾“å‡º
    if sys.platform == 'win32' and hasattr(sys.stdout, 'buffer'):
        sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')
    
    main()

