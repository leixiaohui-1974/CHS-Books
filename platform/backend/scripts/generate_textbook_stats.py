#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç”Ÿæˆæ•™æç»Ÿè®¡æŠ¥å‘Š
åˆ†ææ•™æå†…å®¹è´¨é‡ã€è¦†ç›–åº¦ã€å…³è”æƒ…å†µç­‰
"""
import sys
import io
if sys.platform == 'win32' and hasattr(sys.stdout, 'buffer'):
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')

from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent.parent))

from services.textbook.database import SessionLocal
from services.textbook.models import Textbook, TextbookChapter, ChapterCaseMapping, DifficultyLevel
from sqlalchemy import func
import json

def print_header(text):
    print("\n" + "="*80)
    print(f"  {text}")
    print("="*80 + "\n")

def generate_comprehensive_stats():
    """ç”Ÿæˆç»¼åˆç»Ÿè®¡æŠ¥å‘Š"""
    db = SessionLocal()
    
    try:
        print_header("ğŸ“Š CHS-Books æ•™æç³»ç»Ÿç»¼åˆç»Ÿè®¡æŠ¥å‘Š")
        
        # 1. åŸºç¡€ç»Ÿè®¡
        print("[1] åŸºç¡€æ•°æ®ç»Ÿè®¡")
        print("-" * 60)
        
        total_textbooks = db.query(Textbook).count()
        total_chapters = db.query(TextbookChapter).count()
        total_words = db.query(func.sum(TextbookChapter.word_count)).scalar() or 0
        total_associations = db.query(ChapterCaseMapping).count()
        
        print(f"  ğŸ“– æ•™ææ€»æ•°: {total_textbooks} æœ¬")
        print(f"  ğŸ“„ ç« èŠ‚æ€»æ•°: {total_chapters} ä¸ª")
        print(f"  ğŸ“ æ€»å­—æ•°: {total_words:,} å­—")
        print(f"  ğŸ”— æ•™æ-æ¡ˆä¾‹å…³è”: {total_associations} æ¡")
        
        # 2. æ•™æè§„æ¨¡åˆ†æ
        print("\n[2] æ•™æè§„æ¨¡æ’è¡Œ (Top 10)")
        print("-" * 60)
        
        textbooks = db.query(
            Textbook.title,
            Textbook.total_chapters,
            Textbook.total_words
        ).order_by(Textbook.total_words.desc()).limit(10).all()
        
        for i, (title, chapters, words) in enumerate(textbooks, 1):
            print(f"  {i:2d}. {title[:40]:<40} | {chapters:3d}ç«  | {words:7,d}å­—")
        
        # 3. ç« èŠ‚éš¾åº¦åˆ†å¸ƒ
        print("\n[3] ç« èŠ‚éš¾åº¦åˆ†å¸ƒ")
        print("-" * 60)
        
        difficulty_stats = db.query(
            TextbookChapter.difficulty,
            func.count(TextbookChapter.id)
        ).group_by(TextbookChapter.difficulty).all()
        
        difficulty_map = {
            DifficultyLevel.BEGINNER: "åˆçº§",
            DifficultyLevel.INTERMEDIATE: "ä¸­çº§",
            DifficultyLevel.ADVANCED: "é«˜çº§",
            DifficultyLevel.EXPERT: "ä¸“å®¶",
            None: "æœªåˆ†çº§"
        }
        
        for difficulty, count in difficulty_stats:
            level_name = difficulty_map.get(difficulty, "æœªåˆ†çº§")
            percentage = (count / total_chapters * 100) if total_chapters > 0 else 0
            bar = "â–ˆ" * int(percentage / 2)
            print(f"  {level_name:<8} | {count:4d} ({percentage:5.1f}%) {bar}")
        
        # 4. å†…å®¹ç‰¹å¾ç»Ÿè®¡
        print("\n[4] å†…å®¹ç‰¹å¾ç»Ÿè®¡")
        print("-" * 60)
        
        chapters_with_code = db.query(func.count(TextbookChapter.id)).filter(
            TextbookChapter.has_code == 1
        ).scalar() or 0
        
        chapters_with_formula = db.query(func.count(TextbookChapter.id)).filter(
            TextbookChapter.has_formula == 1
        ).scalar() or 0
        
        chapters_with_image = db.query(func.count(TextbookChapter.id)).filter(
            TextbookChapter.has_image == 1
        ).scalar() or 0
        
        print(f"  ğŸ’» åŒ…å«ä»£ç çš„ç« èŠ‚: {chapters_with_code} ({chapters_with_code/total_chapters*100:.1f}%)")
        print(f"  ğŸ§® åŒ…å«å…¬å¼çš„ç« èŠ‚: {chapters_with_formula} ({chapters_with_formula/total_chapters*100:.1f}%)")
        print(f"  ğŸ–¼ï¸  åŒ…å«å›¾ç‰‡çš„ç« èŠ‚: {chapters_with_image} ({chapters_with_image/total_chapters*100:.1f}%)")
        
        # 5. å…³è”åº¦åˆ†æ
        print("\n[5] æ•™æ-æ¡ˆä¾‹å…³è”åˆ†æ")
        print("-" * 60)
        
        chapters_with_cases = db.query(
            func.count(func.distinct(ChapterCaseMapping.chapter_id))
        ).scalar() or 0
        
        avg_cases_per_chapter = total_associations / total_chapters if total_chapters > 0 else 0
        
        print(f"  æœ‰å…³è”æ¡ˆä¾‹çš„ç« èŠ‚: {chapters_with_cases} / {total_chapters} ({chapters_with_cases/total_chapters*100:.1f}%)")
        print(f"  å¹³å‡æ¯ç« èŠ‚å…³è”æ¡ˆä¾‹æ•°: {avg_cases_per_chapter:.2f} ä¸ª")
        
        # æ‰¾å‡ºå…³è”æœ€å¤šçš„ç« èŠ‚
        top_associated = db.query(
            TextbookChapter.title,
            Textbook.title.label('textbook_title'),
            func.count(ChapterCaseMapping.id).label('case_count')
        ).join(
            ChapterCaseMapping, TextbookChapter.id == ChapterCaseMapping.chapter_id
        ).join(
            Textbook, TextbookChapter.textbook_id == Textbook.id
        ).group_by(
            TextbookChapter.id
        ).order_by(
            func.count(ChapterCaseMapping.id).desc()
        ).limit(5).all()
        
        if top_associated:
            print("\n  å…³è”æ¡ˆä¾‹æœ€å¤šçš„ç« èŠ‚ (Top 5):")
            for i, (chapter_title, textbook_title, count) in enumerate(top_associated, 1):
                print(f"    {i}. {chapter_title[:35]:<35} | {count} ä¸ªæ¡ˆä¾‹ | {textbook_title[:30]}")
        
        # 6. å­—æ•°åˆ†å¸ƒ
        print("\n[6] ç« èŠ‚å­—æ•°åˆ†å¸ƒ")
        print("-" * 60)
        
        word_ranges = [
            (0, 100, "< 100å­—"),
            (100, 500, "100-500å­—"),
            (500, 1000, "500-1,000å­—"),
            (1000, 5000, "1,000-5,000å­—"),
            (5000, 10000, "5,000-10,000å­—"),
            (10000, 999999, "> 10,000å­—")
        ]
        
        for min_words, max_words, label in word_ranges:
            count = db.query(func.count(TextbookChapter.id)).filter(
                TextbookChapter.word_count >= min_words,
                TextbookChapter.word_count < max_words
            ).scalar() or 0
            
            percentage = (count / total_chapters * 100) if total_chapters > 0 else 0
            bar = "â–ˆ" * int(percentage / 2)
            print(f"  {label:<15} | {count:4d} ({percentage:5.1f}%) {bar}")
        
        # 7. è€ƒç ”æ•™æç»Ÿè®¡
        print("\n[7] è€ƒç ”æ•™æä¸“é¡¹ç»Ÿè®¡")
        print("-" * 60)
        
        exam_books = db.query(Textbook).filter(
            Textbook.title.contains("è€ƒç ”")
        ).all()
        
        if exam_books:
            print(f"  è€ƒç ”æ•™ææ•°é‡: {len(exam_books)} æœ¬")
            exam_chapters = sum(b.total_chapters for b in exam_books)
            exam_words = sum(b.total_words for b in exam_books)
            print(f"  è€ƒç ”ç« èŠ‚æ€»æ•°: {exam_chapters} ç« ")
            print(f"  è€ƒç ”å†…å®¹æ€»å­—æ•°: {exam_words:,} å­—")
            
            print("\n  è€ƒç ”æ•™æåˆ—è¡¨:")
            for i, book in enumerate(exam_books, 1):
                print(f"    {i}. {book.title} ({book.total_chapters}ç« , {book.total_words:,}å­—)")
        else:
            print("  æš‚æ— è€ƒç ”æ•™æ")
        
        # 8. è´¨é‡è¯„ä¼°
        print("\n[8] å†…å®¹è´¨é‡è¯„ä¼°")
        print("-" * 60)
        
        # è®¡ç®—è´¨é‡æŒ‡æ ‡
        quality_score = 0
        max_score = 5
        
        # æŒ‡æ ‡1: å†…å®¹å®Œæ•´æ€§ (æœ‰æè¿°ã€å…³é”®è¯ç­‰)
        chapters_with_summary = db.query(func.count(TextbookChapter.id)).filter(
            TextbookChapter.summary.isnot(None)
        ).scalar() or 0
        completeness = chapters_with_summary / total_chapters if total_chapters > 0 else 0
        quality_score += completeness
        
        # æŒ‡æ ‡2: æ¡ˆä¾‹è¦†ç›–ç‡
        coverage = chapters_with_cases / total_chapters if total_chapters > 0 else 0
        quality_score += coverage
        
        # æŒ‡æ ‡3: å¤šæ ·æ€§ (ä»£ç ã€å…¬å¼ã€å›¾ç‰‡)
        diversity = (chapters_with_code + chapters_with_formula + chapters_with_image) / (total_chapters * 3) if total_chapters > 0 else 0
        quality_score += diversity
        
        # æŒ‡æ ‡4: å¹³å‡å­—æ•° (ç†æƒ³èŒƒå›´: 500-5000å­—)
        avg_words = total_words / total_chapters if total_chapters > 0 else 0
        word_quality = 1.0 if 500 <= avg_words <= 5000 else 0.5
        quality_score += word_quality
        
        # æŒ‡æ ‡5: éš¾åº¦åˆ†çº§è¦†ç›–
        difficulty_coverage = len([d for d, c in difficulty_stats if d is not None]) / 4  # 4ä¸ªéš¾åº¦ç­‰çº§
        quality_score += difficulty_coverage
        
        quality_percentage = (quality_score / max_score) * 100
        
        print(f"  å†…å®¹å®Œæ•´æ€§: {completeness*100:.1f}% (ç« èŠ‚æ‘˜è¦è¦†ç›–)")
        print(f"  æ¡ˆä¾‹è¦†ç›–ç‡: {coverage*100:.1f}% (ç« èŠ‚å…³è”æ¡ˆä¾‹)")
        print(f"  å†…å®¹å¤šæ ·æ€§: {diversity*100:.1f}% (ä»£ç /å…¬å¼/å›¾ç‰‡)")
        print(f"  å­—æ•°åˆç†æ€§: {word_quality*100:.1f}% (å¹³å‡{avg_words:.0f}å­—/ç« )")
        print(f"  éš¾åº¦åˆ†çº§: {difficulty_coverage*100:.1f}% (éš¾åº¦ç­‰çº§è¦†ç›–)")
        print(f"\n  ğŸ“Š ç»¼åˆè´¨é‡è¯„åˆ†: {quality_score:.2f}/{max_score} ({quality_percentage:.1f}%)")
        
        quality_level = "ä¼˜ç§€" if quality_percentage >= 80 else "è‰¯å¥½" if quality_percentage >= 60 else "ä¸€èˆ¬" if quality_percentage >= 40 else "éœ€æ”¹è¿›"
        print(f"  è¯„çº§: {quality_level}")
        
        # 9. å»ºè®®æ”¹è¿›
        print("\n[9] æ”¹è¿›å»ºè®®")
        print("-" * 60)
        
        suggestions = []
        
        if completeness < 0.5:
            suggestions.append("â€¢ å¢åŠ ç« èŠ‚æ‘˜è¦å’Œå…³é”®è¯ï¼Œæå‡å†…å®¹å®Œæ•´æ€§")
        
        if coverage < 0.4:
            suggestions.append("â€¢ å¢åŠ æ•™æä¸æ¡ˆä¾‹çš„å…³è”ï¼Œå½“å‰è¦†ç›–ç‡è¾ƒä½")
        
        if chapters_with_code / total_chapters < 0.3:
            suggestions.append("â€¢ å¢åŠ ä»£ç ç¤ºä¾‹ï¼Œæå‡å®è·µæ€§")
        
        if chapters_with_formula / total_chapters < 0.2:
            suggestions.append("â€¢ å¢åŠ å…¬å¼æ¨å¯¼ï¼ŒåŠ å¼ºç†è®ºæ·±åº¦")
        
        if avg_words < 300:
            suggestions.append("â€¢ éƒ¨åˆ†ç« èŠ‚å†…å®¹è¾ƒå°‘ï¼Œå»ºè®®æ‰©å……")
        
        if difficulty_coverage < 0.5:
            suggestions.append("â€¢ å®Œå–„éš¾åº¦åˆ†çº§ï¼Œè¦†ç›–æ‰€æœ‰ç­‰çº§")
        
        if len(exam_books) < 10:
            suggestions.append("â€¢ ç»§ç»­å¼€å‘è€ƒç ”æ•™æï¼Œç›®æ ‡15æœ¬")
        
        if suggestions:
            for suggestion in suggestions:
                print(f"  {suggestion}")
        else:
            print("  âœ… å†…å®¹è´¨é‡ä¼˜ç§€ï¼Œæš‚æ— æ˜æ˜¾æ”¹è¿›ç‚¹")
        
        print_header("ğŸ“‹ æŠ¥å‘Šç”Ÿæˆå®Œæˆ")
        
    finally:
        db.close()

def export_stats_to_json():
    """å¯¼å‡ºç»Ÿè®¡æ•°æ®ä¸ºJSONæ ¼å¼"""
    db = SessionLocal()
    
    try:
        stats = {
            "generated_at": datetime.now().isoformat(),
            "basic_stats": {
                "total_textbooks": db.query(Textbook).count(),
                "total_chapters": db.query(TextbookChapter).count(),
                "total_words": db.query(func.sum(TextbookChapter.word_count)).scalar() or 0,
                "total_associations": db.query(ChapterCaseMapping).count()
            },
            "textbooks": []
        }
        
        textbooks = db.query(Textbook).all()
        for textbook in textbooks:
            stats["textbooks"].append({
                "id": textbook.id,
                "title": textbook.title,
                "total_chapters": textbook.total_chapters,
                "total_words": textbook.total_words,
                "author": textbook.author,
                "version": textbook.version
            })
        
        output_file = Path(__file__).parent.parent / "data" / "textbook_stats.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(stats, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… ç»Ÿè®¡æ•°æ®å·²å¯¼å‡ºåˆ°: {output_file}")
        
    finally:
        db.close()

if __name__ == "__main__":
    from datetime import datetime
    generate_comprehensive_stats()
    
    print("\næ˜¯å¦å¯¼å‡ºJSONæ ¼å¼ç»Ÿè®¡æ•°æ®? (y/n): ", end='')
    # è‡ªåŠ¨å¯¼å‡º
    try:
        export_stats_to_json()
    except Exception as e:
        print(f"å¯¼å‡ºå¤±è´¥: {e}")

