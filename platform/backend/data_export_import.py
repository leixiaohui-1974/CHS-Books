#!/usr/bin/env python3
"""
æ•°æ®å¯¼å…¥å¯¼å‡ºå·¥å…·
æ”¯æŒå¤šç§æ ¼å¼çš„æ•°æ®å¯¼å…¥å¯¼å‡º
"""

import json
import csv
import sys
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any
import zipfile
import shutil


class DataExporter:
    """æ•°æ®å¯¼å‡ºå™¨"""
    
    def __init__(self, output_dir: str = "exports"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
    
    def export_project_structure(self) -> str:
        """å¯¼å‡ºé¡¹ç›®ç»“æ„"""
        print("ğŸ“¦ å¯¼å‡ºé¡¹ç›®ç»“æ„...")
        
        backend_dir = Path(__file__).parent
        platform_dir = backend_dir.parent
        
        structure = {
            'platform': {
                'name': 'intelligent-knowledge-platform',
                'version': 'V2.2.0 Final',
                'export_time': datetime.now().isoformat()
            },
            'directories': {},
            'files': {}
        }
        
        # æ‰«æç›®å½•ç»“æ„
        for item in platform_dir.rglob("*"):
            if '.git' in str(item) or '__pycache__' in str(item) or 'node_modules' in str(item):
                continue
            
            rel_path = item.relative_to(platform_dir)
            
            if item.is_dir():
                structure['directories'][str(rel_path)] = {
                    'files': len(list(item.glob("*")))
                }
            elif item.is_file():
                structure['files'][str(rel_path)] = {
                    'size': item.stat().st_size,
                    'modified': datetime.fromtimestamp(item.stat().st_mtime).isoformat()
                }
        
        # ä¿å­˜
        output_file = self.output_dir / f"project_structure_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(structure, f, indent=2, ensure_ascii=False)
        
        print(f"âœ“ é¡¹ç›®ç»“æ„å·²å¯¼å‡º: {output_file}")
        return str(output_file)
    
    def export_code_statistics(self) -> str:
        """å¯¼å‡ºä»£ç ç»Ÿè®¡"""
        print("ğŸ“Š å¯¼å‡ºä»£ç ç»Ÿè®¡...")
        
        backend_dir = Path(__file__).parent
        
        stats = {
            'export_time': datetime.now().isoformat(),
            'by_extension': {},
            'by_directory': {},
            'total': {
                'files': 0,
                'lines': 0,
                'blank_lines': 0,
                'comment_lines': 0
            }
        }
        
        # ç»Ÿè®¡å„ç±»æ–‡ä»¶
        for ext in ['.py', '.md', '.txt', '.json', '.yml', '.yaml']:
            files = list(backend_dir.parent.rglob(f"*{ext}"))
            files = [f for f in files if '__pycache__' not in str(f) and '.git' not in str(f)]
            
            total_lines = 0
            for file in files:
                try:
                    with open(file, 'r', encoding='utf-8') as f:
                        lines = len(f.readlines())
                        total_lines += lines
                except:
                    pass
            
            if files:
                stats['by_extension'][ext] = {
                    'files': len(files),
                    'lines': total_lines
                }
                stats['total']['files'] += len(files)
                stats['total']['lines'] += total_lines
        
        # ä¿å­˜
        output_file = self.output_dir / f"code_statistics_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(stats, f, indent=2, ensure_ascii=False)
        
        print(f"âœ“ ä»£ç ç»Ÿè®¡å·²å¯¼å‡º: {output_file}")
        return str(output_file)
    
    def export_api_endpoints(self) -> str:
        """å¯¼å‡ºAPIç«¯ç‚¹åˆ—è¡¨"""
        print("ğŸ”Œ å¯¼å‡ºAPIç«¯ç‚¹...")
        
        api_dir = Path(__file__).parent / 'app' / 'api' / 'endpoints'
        
        endpoints = []
        
        if api_dir.exists():
            import re
            
            for api_file in api_dir.glob("*.py"):
                try:
                    with open(api_file, 'r', encoding='utf-8') as f:
                        content = f.read()
                    
                    # æå–è·¯ç”±
                    routes = re.findall(
                        r'@router\.(get|post|put|delete|patch)\(["\']([^"\']+)["\'](?:.*?summary=["\']([^"\']+))?',
                        content,
                        re.DOTALL
                    )
                    
                    for method, path, summary in routes:
                        endpoints.append({
                            'file': api_file.name,
                            'method': method.upper(),
                            'path': path,
                            'summary': summary if summary else ''
                        })
                
                except Exception as e:
                    pass
        
        # ä¿å­˜ä¸ºJSON
        json_file = self.output_dir / f"api_endpoints_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(endpoints, f, indent=2, ensure_ascii=False)
        
        # ä¿å­˜ä¸ºCSV
        csv_file = self.output_dir / f"api_endpoints_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
        with open(csv_file, 'w', newline='', encoding='utf-8') as f:
            writer = csv.DictWriter(f, fieldnames=['file', 'method', 'path', 'summary'])
            writer.writeheader()
            writer.writerows(endpoints)
        
        print(f"âœ“ APIç«¯ç‚¹å·²å¯¼å‡º: {json_file}")
        print(f"âœ“ APIç«¯ç‚¹å·²å¯¼å‡º: {csv_file}")
        return str(json_file)
    
    def export_documentation(self) -> str:
        """å¯¼å‡ºæ–‡æ¡£åˆ—è¡¨"""
        print("ğŸ“š å¯¼å‡ºæ–‡æ¡£åˆ—è¡¨...")
        
        platform_dir = Path(__file__).parent.parent
        
        docs = []
        
        for md_file in platform_dir.rglob("*.md"):
            if '.git' in str(md_file):
                continue
            
            try:
                with open(md_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                    lines = content.split('\n')
                    
                    # æå–ç¬¬ä¸€ä¸ªæ ‡é¢˜
                    title = ''
                    for line in lines:
                        if line.startswith('# '):
                            title = line[2:].strip()
                            break
                
                docs.append({
                    'file': str(md_file.relative_to(platform_dir)),
                    'title': title,
                    'size_kb': md_file.stat().st_size / 1024,
                    'lines': len(lines),
                    'modified': datetime.fromtimestamp(md_file.stat().st_mtime).isoformat()
                })
            
            except Exception:
                pass
        
        # æŒ‰æ–‡ä»¶å¤§å°æ’åº
        docs.sort(key=lambda x: x['size_kb'], reverse=True)
        
        # ä¿å­˜
        output_file = self.output_dir / f"documentation_list_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(docs, f, indent=2, ensure_ascii=False)
        
        print(f"âœ“ æ–‡æ¡£åˆ—è¡¨å·²å¯¼å‡º: {output_file}")
        return str(output_file)
    
    def create_backup(self) -> str:
        """åˆ›å»ºå®Œæ•´å¤‡ä»½"""
        print("ğŸ’¾ åˆ›å»ºé¡¹ç›®å¤‡ä»½...")
        
        platform_dir = Path(__file__).parent.parent
        backup_name = f"platform_backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}.zip"
        backup_path = self.output_dir / backup_name
        
        with zipfile.ZipFile(backup_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
            # å…³é”®ç›®å½•
            important_dirs = [
                'backend/app',
                'backend/*.py',
                'sdk',
                'examples',
                '*.md',
                'docker-compose*.yml'
            ]
            
            file_count = 0
            
            # æ·»åŠ Pythonæ–‡ä»¶
            for py_file in platform_dir.rglob("*.py"):
                if '__pycache__' not in str(py_file) and '.git' not in str(py_file):
                    arcname = py_file.relative_to(platform_dir)
                    zipf.write(py_file, arcname)
                    file_count += 1
            
            # æ·»åŠ æ–‡æ¡£
            for md_file in platform_dir.glob("*.md"):
                arcname = md_file.relative_to(platform_dir)
                zipf.write(md_file, arcname)
                file_count += 1
            
            # æ·»åŠ é…ç½®æ–‡ä»¶
            for config in ['docker-compose.v2.yml', 'backend/requirements.txt']:
                config_file = platform_dir / config
                if config_file.exists():
                    arcname = config_file.relative_to(platform_dir)
                    zipf.write(config_file, arcname)
                    file_count += 1
        
        size_mb = backup_path.stat().st_size / (1024 ** 2)
        print(f"âœ“ å¤‡ä»½å·²åˆ›å»º: {backup_path} ({size_mb:.2f}MB, {file_count}ä¸ªæ–‡ä»¶)")
        return str(backup_path)


class DataImporter:
    """æ•°æ®å¯¼å…¥å™¨"""
    
    def __init__(self):
        pass
    
    def import_from_json(self, json_file: str) -> Dict[str, Any]:
        """ä»JSONå¯¼å…¥"""
        print(f"ğŸ“¥ ä»JSONå¯¼å…¥: {json_file}")
        
        with open(json_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        print(f"âœ“ å¯¼å…¥å®Œæˆï¼ŒåŒ…å«{len(data)}æ¡è®°å½•")
        return data
    
    def import_from_csv(self, csv_file: str) -> List[Dict[str, Any]]:
        """ä»CSVå¯¼å…¥"""
        print(f"ğŸ“¥ ä»CSVå¯¼å…¥: {csv_file}")
        
        data = []
        with open(csv_file, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            data = list(reader)
        
        print(f"âœ“ å¯¼å…¥å®Œæˆï¼ŒåŒ…å«{len(data)}æ¡è®°å½•")
        return data
    
    def restore_from_backup(self, backup_file: str, target_dir: str = None):
        """ä»å¤‡ä»½æ¢å¤"""
        print(f"ğŸ“¥ ä»å¤‡ä»½æ¢å¤: {backup_file}")
        
        if target_dir is None:
            target_dir = Path(__file__).parent.parent / "restored"
        else:
            target_dir = Path(target_dir)
        
        target_dir.mkdir(exist_ok=True)
        
        with zipfile.ZipFile(backup_file, 'r') as zipf:
            zipf.extractall(target_dir)
            file_count = len(zipf.namelist())
        
        print(f"âœ“ æ¢å¤å®Œæˆï¼Œå…±{file_count}ä¸ªæ–‡ä»¶")
        print(f"  ç›®æ ‡ç›®å½•: {target_dir}")
        return str(target_dir)


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='æ•°æ®å¯¼å…¥å¯¼å‡ºå·¥å…·')
    parser.add_argument('command', 
                       choices=['export', 'import', 'backup', 'restore'],
                       help='æ“ä½œå‘½ä»¤')
    parser.add_argument('--type', 
                       choices=['structure', 'stats', 'api', 'docs', 'all'],
                       default='all',
                       help='å¯¼å‡ºç±»å‹')
    parser.add_argument('--file', help='å¯¼å…¥/æ¢å¤çš„æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--format', choices=['json', 'csv'], default='json', help='æ–‡ä»¶æ ¼å¼')
    parser.add_argument('--target', help='æ¢å¤ç›®æ ‡ç›®å½•')
    
    args = parser.parse_args()
    
    print()
    print("=" * 70)
    print(" æ•°æ®å¯¼å…¥å¯¼å‡ºå·¥å…·")
    print("=" * 70)
    print()
    
    if args.command == 'export':
        exporter = DataExporter()
        
        if args.type in ['structure', 'all']:
            exporter.export_project_structure()
        
        if args.type in ['stats', 'all']:
            exporter.export_code_statistics()
        
        if args.type in ['api', 'all']:
            exporter.export_api_endpoints()
        
        if args.type in ['docs', 'all']:
            exporter.export_documentation()
        
        print()
        print(f"âœ… å¯¼å‡ºå®Œæˆï¼æ–‡ä»¶ä¿å­˜åœ¨: {exporter.output_dir}")
    
    elif args.command == 'backup':
        exporter = DataExporter()
        backup_file = exporter.create_backup()
        print()
        print(f"âœ… å¤‡ä»½å®Œæˆï¼æ–‡ä»¶: {backup_file}")
    
    elif args.command == 'import':
        if not args.file:
            print("âŒ é”™è¯¯: éœ€è¦æŒ‡å®š--fileå‚æ•°")
            sys.exit(1)
        
        importer = DataImporter()
        
        if args.format == 'json':
            data = importer.import_from_json(args.file)
        elif args.format == 'csv':
            data = importer.import_from_csv(args.file)
        
        print()
        print(f"âœ… å¯¼å…¥å®Œæˆï¼")
    
    elif args.command == 'restore':
        if not args.file:
            print("âŒ é”™è¯¯: éœ€è¦æŒ‡å®š--fileå‚æ•°")
            sys.exit(1)
        
        importer = DataImporter()
        target = importer.restore_from_backup(args.file, args.target)
        print()
        print(f"âœ… æ¢å¤å®Œæˆï¼ä½ç½®: {target}")
    
    print()


if __name__ == "__main__":
    main()
