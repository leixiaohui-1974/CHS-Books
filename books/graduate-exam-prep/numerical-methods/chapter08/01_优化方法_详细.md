# ç¬¬8ç« ï¼šä¼˜åŒ–æ–¹æ³•

**å­¦ä¹ æ—¶é—´**: 4å°æ—¶  
**è€ƒè¯•é¢‘ç‡**: â­â­â­â­â­  
**éš¾åº¦**: â­â­â­â­â­

---

## ä¸€ã€ä¼˜åŒ–é—®é¢˜åŸºç¡€

### 1.1 é—®é¢˜åˆ†ç±»

**æ— çº¦æŸä¼˜åŒ–**ï¼š
$$\min_{x \in \mathbb{R}^n} f(x)$$

**çº¦æŸä¼˜åŒ–**ï¼š
$$\begin{aligned}
\min_{x \in \mathbb{R}^n} \quad & f(x) \\
\text{s.t.} \quad & g_i(x) \leq 0, \quad i=1,\ldots,m \\
& h_j(x) = 0, \quad j=1,\ldots,p
\end{aligned}$$

### 1.2 æœ€ä¼˜æ€§æ¡ä»¶

**ä¸€é˜¶å¿…è¦æ¡ä»¶**ï¼ˆæ¢¯åº¦ä¸ºé›¶ï¼‰ï¼š
$$\nabla f(x^*) = 0$$

**äºŒé˜¶å……åˆ†æ¡ä»¶**ï¼ˆHessianæ­£å®šï¼‰ï¼š
$$\nabla f(x^*) = 0, \quad \nabla^2 f(x^*) \succ 0$$

**HessiançŸ©é˜µ**ï¼š
$$H = \nabla^2 f = \begin{bmatrix}
\frac{\partial^2 f}{\partial x_1^2} & \cdots & \frac{\partial^2 f}{\partial x_1 \partial x_n} \\
\vdots & \ddots & \vdots \\
\frac{\partial^2 f}{\partial x_n \partial x_1} & \cdots & \frac{\partial^2 f}{\partial x_n^2}
\end{bmatrix}$$

---

## äºŒã€æ— çº¦æŸä¼˜åŒ–æ–¹æ³•

### 2.1 æœ€é€Ÿä¸‹é™æ³•ï¼ˆSteepest Descentï¼‰

**åŸºæœ¬æ€æƒ³**ï¼šæ²¿è´Ÿæ¢¯åº¦æ–¹å‘æœç´¢

**è¿­ä»£æ ¼å¼**ï¼š
$$x^{(k+1)} = x^{(k)} - \alpha_k \nabla f(x^{(k)})$$

**æ­¥é•¿é€‰æ‹©**ï¼š
- å›ºå®šæ­¥é•¿ï¼š$\alpha_k = \alpha$
- ç²¾ç¡®çº¿æœç´¢ï¼š$\alpha_k = \arg\min_{\alpha} f(x^{(k)} - \alpha \nabla f(x^{(k)}))$
- Armijoå‡†åˆ™ï¼ˆå›æº¯çº¿æœç´¢ï¼‰

**æ”¶æ•›æ€§**ï¼š
- çº¿æ€§æ”¶æ•›
- é”¯é½¿ç°è±¡ï¼ˆæ¥è¿‘æœ€ä¼˜è§£æ—¶ï¼‰

**ä¾‹é¢˜1**ï¼šæœ€é€Ÿä¸‹é™æ³•æ±‚$f(x,y) = x^2 + 4y^2$çš„æœ€å°å€¼

**è§£**ï¼š
æ¢¯åº¦ï¼š$\nabla f = [2x, 8y]^T$

å–åˆå€¼$(x^{(0)}, y^{(0)}) = (4, 2)$

ç¬¬1æ¬¡è¿­ä»£ï¼š
$$\nabla f(4,2) = [8, 16]^T$$

ç²¾ç¡®çº¿æœç´¢ï¼š
$$f(4-8\alpha, 2-16\alpha) = (4-8\alpha)^2 + 4(2-16\alpha)^2$$

æ±‚å¯¼å¾—ï¼š$\alpha^* = \frac{5}{17}$

$$x^{(1)} = 4 - 8 \cdot \frac{5}{17} = \frac{28}{17}, \quad y^{(1)} = 2 - 16 \cdot \frac{5}{17} = -\frac{46}{17}$$

---

### 2.2 å…±è½­æ¢¯åº¦æ³•ï¼ˆConjugate Gradient Methodï¼‰

**åŠ¨æœº**ï¼šå…‹æœæœ€é€Ÿä¸‹é™æ³•çš„é”¯é½¿ç°è±¡

**å…±è½­æ–¹å‘**ï¼š
$$d_i^T A d_j = 0, \quad i \neq j$$

**è¿­ä»£æ ¼å¼**ï¼š
$$x^{(k+1)} = x^{(k)} + \alpha_k d^{(k)}$$

å…¶ä¸­æ–¹å‘$d^{(k)}$æ»¡è¶³ï¼š
$$d^{(k)} = -\nabla f(x^{(k)}) + \beta_k d^{(k-1)}$$

**$\beta_k$è®¡ç®—**ï¼ˆå¤šç§å…¬å¼ï¼‰ï¼š

**Fletcher-Reeves (FR)**ï¼š
$$\beta_k^{FR} = \frac{\|\nabla f(x^{(k)})\|^2}{\|\nabla f(x^{(k-1)})\|^2}$$

**Polak-RibiÃ¨re (PR)**ï¼š
$$\beta_k^{PR} = \frac{\nabla f(x^{(k)})^T (\nabla f(x^{(k)}) - \nabla f(x^{(k-1)}))}{\|\nabla f(x^{(k-1)})\|^2}$$

**æ€§è´¨**ï¼š
- äºŒæ¬¡å‡½æ•°ï¼š$n$æ­¥æ”¶æ•›åˆ°ç²¾ç¡®è§£
- ä¸€èˆ¬å‡½æ•°ï¼šè¶…çº¿æ€§æ”¶æ•›

---

### 2.3 ç‰›é¡¿æ³•

**åŸºæœ¬æ€æƒ³**ï¼šäºŒæ¬¡è¿‘ä¼¼

**æ³°å‹’å±•å¼€**ï¼š
$$f(x + \Delta x) \approx f(x) + \nabla f(x)^T \Delta x + \frac{1}{2}\Delta x^T \nabla^2 f(x) \Delta x$$

**æœ€ä¼˜æ€§æ¡ä»¶**ï¼š
$$\nabla^2 f(x) \Delta x = -\nabla f(x)$$

**è¿­ä»£æ ¼å¼**ï¼š
$$x^{(k+1)} = x^{(k)} - [\nabla^2 f(x^{(k)})]^{-1} \nabla f(x^{(k)})$$

**æ”¶æ•›æ€§**ï¼š
- å±€éƒ¨äºŒæ¬¡æ”¶æ•›
- éœ€Hessianæ­£å®š

**ç¼ºç‚¹**ï¼š
- è®¡ç®—Hessianï¼š$O(n^2)$
- æ±‚è§£çº¿æ€§æ–¹ç¨‹ç»„ï¼š$O(n^3)$

---

### 2.4 æ‹Ÿç‰›é¡¿æ³•ï¼ˆBFGSï¼‰

**åŠ¨æœº**ï¼šé¿å…è®¡ç®—Hessian

**æ€æƒ³**ï¼šç”¨çŸ©é˜µ$B_k$è¿‘ä¼¼$\nabla^2 f$

**BFGSæ›´æ–°å…¬å¼**ï¼š
$$B_{k+1} = B_k - \frac{B_k s_k s_k^T B_k}{s_k^T B_k s_k} + \frac{y_k y_k^T}{y_k^T s_k}$$

å…¶ä¸­ï¼š
$$s_k = x^{(k+1)} - x^{(k)}, \quad y_k = \nabla f(x^{(k+1)}) - \nabla f(x^{(k)})$$

**è¿­ä»£æ ¼å¼**ï¼š
$$x^{(k+1)} = x^{(k)} - \alpha_k B_k^{-1} \nabla f(x^{(k)})$$

**æ”¶æ•›æ€§**ï¼šè¶…çº¿æ€§æ”¶æ•›

---

## ä¸‰ã€çº¦æŸä¼˜åŒ–æ–¹æ³•

### 3.1 KKTæ¡ä»¶

**Lagrangeå‡½æ•°**ï¼š
$$L(x, \lambda, \mu) = f(x) + \sum_{i=1}^m \lambda_i g_i(x) + \sum_{j=1}^p \mu_j h_j(x)$$

**KKTæ¡ä»¶**ï¼š
$$\begin{aligned}
\nabla_x L &= 0 \\
g_i(x) &\leq 0, \quad i=1,\ldots,m \\
h_j(x) &= 0, \quad j=1,\ldots,p \\
\lambda_i &\geq 0, \quad i=1,\ldots,m \\
\lambda_i g_i(x) &= 0, \quad i=1,\ldots,m
\end{aligned}$$

### 3.2 ç½šå‡½æ•°æ³•

**åŸºæœ¬æ€æƒ³**ï¼šå°†çº¦æŸé—®é¢˜è½¬åŒ–ä¸ºæ— çº¦æŸé—®é¢˜

**å¤–ç½šå‡½æ•°**ï¼š
$$\min P(x, \rho) = f(x) + \rho \sum_{i=1}^m \max(0, g_i(x))^2 + \rho \sum_{j=1}^p h_j(x)^2$$

**ç®—æ³•**ï¼š
1. åˆå§‹åŒ–$\rho_0$ï¼ˆè¾ƒå°ï¼‰
2. æ±‚è§£æ— çº¦æŸé—®é¢˜$\min P(x, \rho_k)$
3. å¢å¤§$\rho_{k+1} = c\rho_k$ï¼ˆ$c>1$ï¼Œå¦‚$c=10$ï¼‰
4. é‡å¤ç›´åˆ°æ”¶æ•›

**ä¾‹é¢˜2**ï¼šç½šå‡½æ•°æ³•æ±‚$\min x^2+y^2$ï¼Œçº¦æŸ$x+y \geq 1$

**è§£**ï¼š
ç­‰ä»·äº$\min x^2+y^2$ï¼Œçº¦æŸ$1-x-y \leq 0$

ç½šå‡½æ•°ï¼š
$$P(x,y,\rho) = x^2+y^2 + \rho \max(0, 1-x-y)^2$$

å–$\rho=10$ï¼š
- è‹¥$x+y<1$ï¼š$P = x^2+y^2 + 10(1-x-y)^2$
- è‹¥$x+y \geq 1$ï¼š$P = x^2+y^2$

æ±‚è§£æ— çº¦æŸé—®é¢˜...

---

### 3.3 çº¿æ€§è§„åˆ’ï¼ˆå•çº¯å½¢æ³•ï¼‰

**æ ‡å‡†å½¢å¼**ï¼š
$$\begin{aligned}
\min \quad & c^T x \\
\text{s.t.} \quad & Ax = b \\
& x \geq 0
\end{aligned}$$

**å•çº¯å½¢æ³•åŸºæœ¬æ€æƒ³**ï¼š
- å¯è¡ŒåŸŸä¸ºå‡¸å¤šé¢ä½“
- æœ€ä¼˜è§£åœ¨é¡¶ç‚¹
- ä»ä¸€ä¸ªé¡¶ç‚¹ç§»åŠ¨åˆ°ç›¸é‚»æ›´ä¼˜çš„é¡¶ç‚¹

**ç®—æ³•æ­¥éª¤**ï¼š
1. å¯»æ‰¾åˆå§‹åŸºå¯è¡Œè§£
2. åˆ¤æ–­æœ€ä¼˜æ€§ï¼ˆæ£€éªŒæ•°$\leq 0$ï¼‰
3. é€‰æ‹©è¿›åŸºå˜é‡ï¼ˆæœ€å°æ£€éªŒæ•°ï¼‰
4. é€‰æ‹©å‡ºåŸºå˜é‡ï¼ˆæœ€å°æ¯”å€¼è§„åˆ™ï¼‰
5. æ¢åŸºï¼Œæ›´æ–°å•çº¯å½¢è¡¨
6. é‡å¤2-5

---

## å››ã€å…¨å±€ä¼˜åŒ–æ–¹æ³•

### 4.1 é—ä¼ ç®—æ³•ï¼ˆGenetic Algorithmï¼‰

**åŸºæœ¬æ­¥éª¤**ï¼š
1. **åˆå§‹åŒ–**ï¼šéšæœºç”Ÿæˆç§ç¾¤
2. **é€‚åº”åº¦è¯„ä»·**ï¼šè®¡ç®—$f(x)$
3. **é€‰æ‹©**ï¼šè½®ç›˜èµŒ/é”¦æ ‡èµ›é€‰æ‹©
4. **äº¤å‰**ï¼šå•ç‚¹/å¤šç‚¹/å‡åŒ€äº¤å‰
5. **å˜å¼‚**ï¼šä»¥å°æ¦‚ç‡éšæœºæ”¹å˜
6. **è¿­ä»£**ï¼šé‡å¤2-5ç›´åˆ°æ”¶æ•›

**ç¼–ç **ï¼š
- äºŒè¿›åˆ¶ç¼–ç 
- å®æ•°ç¼–ç 

---

### 4.2 æ¨¡æ‹Ÿé€€ç«ï¼ˆSimulated Annealingï¼‰

**ç‰©ç†ç±»æ¯”**ï¼šå›ºä½“é€€ç«è¿‡ç¨‹

**Metropoliså‡†åˆ™**ï¼š
$$P(\text{æ¥å—}) = \begin{cases}
1, & \Delta E < 0 \\
e^{-\Delta E/T}, & \Delta E \geq 0
\end{cases}$$

**ç®—æ³•æ­¥éª¤**ï¼š
1. åˆå§‹åŒ–$x^{(0)}$ï¼Œ$T_0$ï¼ˆé«˜æ¸©ï¼‰
2. åœ¨å½“å‰è§£é‚»åŸŸç”Ÿæˆæ–°è§£$x'$
3. è®¡ç®—$\Delta E = f(x') - f(x^{(k)})$
4. æŒ‰Metropoliså‡†åˆ™å†³å®šæ˜¯å¦æ¥å—
5. é™æ¸©ï¼š$T_{k+1} = \alpha T_k$ï¼ˆ$\alpha \approx 0.9$ï¼‰
6. é‡å¤2-5ç›´åˆ°æ¸©åº¦å……åˆ†ä½

---

## äº”ã€Pythonå®ç°

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy.optimize import minimize, linprog

class OptimizationMethods:
    """ä¼˜åŒ–æ–¹æ³•å·¥å…·ç±»"""
    
    @staticmethod
    def steepest_descent(f, grad_f, x0, alpha=0.1, tol=1e-6, max_iter=1000):
        """
        æœ€é€Ÿä¸‹é™æ³•
        
        å‚æ•°:
            f: ç›®æ ‡å‡½æ•°
            grad_f: æ¢¯åº¦å‡½æ•°
            x0: åˆå€¼
            alpha: æ­¥é•¿
            tol: å®¹å·®
            max_iter: æœ€å¤§è¿­ä»£æ¬¡æ•°
        """
        x = x0.copy()
        history = [x0.copy()]
        f_history = [f(x0)]
        
        for k in range(max_iter):
            grad = grad_f(x)
            
            # çº¿æœç´¢ï¼ˆArmijoå‡†åˆ™ï¼‰
            alpha_k = alpha
            beta = 0.5
            sigma = 0.1
            
            while f(x - alpha_k * grad) > f(x) - sigma * alpha_k * np.linalg.norm(grad)**2:
                alpha_k *= beta
            
            # æ›´æ–°
            x_new = x - alpha_k * grad
            
            history.append(x_new.copy())
            f_history.append(f(x_new))
            
            # æ”¶æ•›åˆ¤æ–­
            if np.linalg.norm(grad) < tol:
                print(f"æœ€é€Ÿä¸‹é™æ³•æ”¶æ•›äºç¬¬{k+1}æ¬¡è¿­ä»£")
                break
            
            x = x_new
        
        return x, np.array(history), f_history
    
    @staticmethod
    def conjugate_gradient(f, grad_f, x0, tol=1e-6, max_iter=1000):
        """
        å…±è½­æ¢¯åº¦æ³•ï¼ˆPolak-RibiÃ¨reï¼‰
        
        å‚æ•°:
            f: ç›®æ ‡å‡½æ•°
            grad_f: æ¢¯åº¦å‡½æ•°
            x0: åˆå€¼
        """
        x = x0.copy()
        history = [x0.copy()]
        f_history = [f(x0)]
        
        grad = grad_f(x)
        d = -grad.copy()  # åˆå§‹æœç´¢æ–¹å‘
        
        for k in range(max_iter):
            # çº¿æœç´¢
            from scipy.optimize import line_search
            
            alpha = line_search(f, grad_f, x, d)[0]
            if alpha is None:
                alpha = 0.01
            
            # æ›´æ–°
            x_new = x + alpha * d
            grad_new = grad_f(x_new)
            
            history.append(x_new.copy())
            f_history.append(f(x_new))
            
            # æ”¶æ•›åˆ¤æ–­
            if np.linalg.norm(grad_new) < tol:
                print(f"å…±è½­æ¢¯åº¦æ³•æ”¶æ•›äºç¬¬{k+1}æ¬¡è¿­ä»£")
                break
            
            # Polak-RibiÃ¨reå…¬å¼
            beta = max(0, np.dot(grad_new, grad_new - grad) / np.dot(grad, grad))
            
            # æ›´æ–°æœç´¢æ–¹å‘
            d = -grad_new + beta * d
            
            x = x_new
            grad = grad_new
        
        return x, np.array(history), f_history
    
    @staticmethod
    def bfgs(f, grad_f, x0, tol=1e-6, max_iter=1000):
        """
        BFGSæ‹Ÿç‰›é¡¿æ³•
        
        å‚æ•°:
            f: ç›®æ ‡å‡½æ•°
            grad_f: æ¢¯åº¦å‡½æ•°
            x0: åˆå€¼
        """
        n = len(x0)
        x = x0.copy()
        history = [x0.copy()]
        f_history = [f(x0)]
        
        # åˆå§‹åŒ–Hessiané€†çŸ©é˜µè¿‘ä¼¼
        H_inv = np.eye(n)
        
        for k in range(max_iter):
            grad = grad_f(x)
            
            # æœç´¢æ–¹å‘
            d = -H_inv @ grad
            
            # çº¿æœç´¢
            from scipy.optimize import line_search
            alpha = line_search(f, grad_f, x, d)[0]
            if alpha is None:
                alpha = 0.01
            
            # æ›´æ–°
            x_new = x + alpha * d
            grad_new = grad_f(x_new)
            
            history.append(x_new.copy())
            f_history.append(f(x_new))
            
            # æ”¶æ•›åˆ¤æ–­
            if np.linalg.norm(grad_new) < tol:
                print(f"BFGSæ”¶æ•›äºç¬¬{k+1}æ¬¡è¿­ä»£")
                break
            
            # BFGSæ›´æ–°
            s = x_new - x
            y = grad_new - grad
            
            rho = 1.0 / (y.T @ s)
            
            if rho > 0:  # ç¡®ä¿æ­£å®šæ€§
                V = np.eye(n) - rho * np.outer(s, y)
                H_inv = V @ H_inv @ V.T + rho * np.outer(s, s)
            
            x = x_new
        
        return x, np.array(history), f_history
    
    @staticmethod
    def penalty_method(f, constraints, x0, rho_init=1, rho_factor=10,
                      tol=1e-6, max_outer=20):
        """
        å¤–ç½šå‡½æ•°æ³•
        
        å‚æ•°:
            f: ç›®æ ‡å‡½æ•°
            constraints: çº¦æŸå‡½æ•°åˆ—è¡¨ï¼ˆg(x) <= 0ï¼‰
            x0: åˆå€¼
            rho_init: åˆå§‹ç½šå‚æ•°
            rho_factor: ç½šå‚æ•°å¢é•¿å› å­
        """
        x = x0.copy()
        rho = rho_init
        
        for k in range(max_outer):
            # å®šä¹‰ç½šå‡½æ•°
            def P(x_var):
                penalty = sum(max(0, g(x_var))**2 for g in constraints)
                return f(x_var) + rho * penalty
            
            # æ±‚è§£æ— çº¦æŸé—®é¢˜
            result = minimize(P, x, method='BFGS')
            x_new = result.x
            
            # æ£€æŸ¥çº¦æŸè¿å
            violation = sum(max(0, g(x_new)) for g in constraints)
            
            print(f"  å¤–è¿­ä»£{k+1}: Ï={rho:.1e}, è¿ååº¦={violation:.2e}")
            
            if violation < tol:
                print(f"ç½šå‡½æ•°æ³•æ”¶æ•›")
                break
            
            x = x_new
            rho *= rho_factor
        
        return x
    
    @staticmethod
    def plot_optimization_path_2d(f, history, title='ä¼˜åŒ–è·¯å¾„'):
        """ç»˜åˆ¶äºŒç»´ä¼˜åŒ–è·¯å¾„"""
        fig, axes = plt.subplots(1, 2, figsize=(14, 6))
        
        # ç­‰å€¼çº¿+è·¯å¾„
        ax1 = axes[0]
        
        x_range = [history[:,0].min()-1, history[:,0].max()+1]
        y_range = [history[:,1].min()-1, history[:,1].max()+1]
        
        x = np.linspace(x_range[0], x_range[1], 100)
        y = np.linspace(y_range[0], y_range[1], 100)
        X, Y = np.meshgrid(x, y)
        Z = np.zeros_like(X)
        
        for i in range(X.shape[0]):
            for j in range(X.shape[1]):
                Z[i,j] = f(np.array([X[i,j], Y[i,j]]))
        
        contour = ax1.contour(X, Y, Z, levels=20, cmap='viridis', alpha=0.6)
        ax1.clabel(contour, inline=True, fontsize=8)
        
        # ä¼˜åŒ–è·¯å¾„
        ax1.plot(history[:,0], history[:,1], 'r-o', linewidth=2,
                markersize=5, label='ä¼˜åŒ–è·¯å¾„')
        ax1.plot(history[0,0], history[0,1], 'gs', markersize=12,
                label='åˆå€¼')
        ax1.plot(history[-1,0], history[-1,1], 'r*', markersize=15,
                label='ç»ˆå€¼')
        
        ax1.set_xlabel('xâ‚', fontsize=11)
        ax1.set_ylabel('xâ‚‚', fontsize=11)
        ax1.set_title('ä¼˜åŒ–è·¯å¾„ï¼ˆç­‰å€¼çº¿å›¾ï¼‰', fontweight='bold')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # å‡½æ•°å€¼æ”¶æ•›
        ax2 = axes[1]
        
        f_vals = [f(h) for h in history]
        
        ax2.semilogy(range(len(f_vals)), f_vals, 'b-o', linewidth=2,
                    markersize=5)
        ax2.set_xlabel('è¿­ä»£æ¬¡æ•°', fontsize=11)
        ax2.set_ylabel('f(x)', fontsize=11)
        ax2.set_title('ç›®æ ‡å‡½æ•°æ”¶æ•›æ›²çº¿', fontweight='bold')
        ax2.grid(True, alpha=0.3, which='both')
        
        plt.tight_layout()
        plt.savefig('optimization_path.png', dpi=300)
        plt.show()

# ç¤ºä¾‹ä½¿ç”¨
print("="*60)
print("ä¼˜åŒ–æ–¹æ³•æ¼”ç¤º")
print("="*60)

om = OptimizationMethods()

# æµ‹è¯•å‡½æ•°ï¼šRosenbrockå‡½æ•°
def rosenbrock(x):
    return (1 - x[0])**2 + 100*(x[1] - x[0]**2)**2

def grad_rosenbrock(x):
    return np.array([
        -2*(1 - x[0]) - 400*x[0]*(x[1] - x[0]**2),
        200*(x[1] - x[0]**2)
    ])

x0 = np.array([-1.2, 1.0])

print("\næµ‹è¯•å‡½æ•°: Rosenbrockå‡½æ•°")
print(f"  f(x,y) = (1-x)Â² + 100(y-xÂ²)Â²")
print(f"  æœ€ä¼˜è§£: x* = [1, 1], f(x*) = 0")
print(f"  åˆå€¼: x0 = {x0}")

# 1. æœ€é€Ÿä¸‹é™æ³•
print("\n" + "="*60)
print("1. æœ€é€Ÿä¸‹é™æ³•")
print("="*60)

x_sd, history_sd, f_history_sd = om.steepest_descent(
    rosenbrock, grad_rosenbrock, x0)

print(f"  è§£ = [{x_sd[0]:.6f}, {x_sd[1]:.6f}]")
print(f"  f(x) = {rosenbrock(x_sd):.2e}")
print(f"  è¿­ä»£æ¬¡æ•° = {len(history_sd)-1}")

# 2. å…±è½­æ¢¯åº¦æ³•
print("\n" + "="*60)
print("2. å…±è½­æ¢¯åº¦æ³•")
print("="*60)

x_cg, history_cg, f_history_cg = om.conjugate_gradient(
    rosenbrock, grad_rosenbrock, x0)

print(f"  è§£ = [{x_cg[0]:.6f}, {x_cg[1]:.6f}]")
print(f"  f(x) = {rosenbrock(x_cg):.2e}")
print(f"  è¿­ä»£æ¬¡æ•° = {len(history_cg)-1}")

# å¯è§†åŒ–
om.plot_optimization_path_2d(rosenbrock, history_cg, 'å…±è½­æ¢¯åº¦æ³•ä¼˜åŒ–è·¯å¾„')

# 3. BFGS
print("\n" + "="*60)
print("3. BFGSæ‹Ÿç‰›é¡¿æ³•")
print("="*60)

x_bfgs, history_bfgs, f_history_bfgs = om.bfgs(
    rosenbrock, grad_rosenbrock, x0)

print(f"  è§£ = [{x_bfgs[0]:.6f}, {x_bfgs[1]:.6f}]")
print(f"  f(x) = {rosenbrock(x_bfgs):.2e}")
print(f"  è¿­ä»£æ¬¡æ•° = {len(history_bfgs)-1}")

# 4. ç½šå‡½æ•°æ³•ç¤ºä¾‹
print("\n" + "="*60)
print("4. ç½šå‡½æ•°æ³•ï¼ˆçº¦æŸä¼˜åŒ–ï¼‰")
print("="*60)

print("é—®é¢˜: min xÂ²+yÂ², s.t. x+yâ‰¥1")

def f_constrained(x):
    return x[0]**2 + x[1]**2

constraints_list = [
    lambda x: 1 - x[0] - x[1]  # 1-x-y <= 0 å³ x+y >= 1
]

x0_const = np.array([0.0, 0.0])
x_penalty = om.penalty_method(f_constrained, constraints_list, x0_const)

print(f"  è§£ = [{x_penalty[0]:.6f}, {x_penalty[1]:.6f}]")
print(f"  f(x) = {f_constrained(x_penalty):.6f}")
print(f"  çº¦æŸæ£€éªŒ: x+y = {x_penalty[0]+x_penalty[1]:.6f} (â‰¥1?)")

# æ–¹æ³•å¯¹æ¯”
print("\n" + "="*60)
print("æ–¹æ³•å¯¹æ¯”ï¼ˆRosenbrockå‡½æ•°ï¼‰")
print("="*60)
print(f"{'æ–¹æ³•':<15} {'è¿­ä»£æ¬¡æ•°':<10} {'æœ€ç»ˆå‡½æ•°å€¼':<15}")
print("-"*40)
print(f"{'æœ€é€Ÿä¸‹é™æ³•':<15} {len(history_sd)-1:<10} {rosenbrock(x_sd):.2e}")
print(f"{'å…±è½­æ¢¯åº¦æ³•':<15} {len(history_cg)-1:<10} {rosenbrock(x_cg):.2e}")
print(f"{'BFGS':<15} {len(history_bfgs)-1:<10} {rosenbrock(x_bfgs):.2e}")
```

---

## å…­ã€æ–¹æ³•æ€»ç»“

### æ— çº¦æŸä¼˜åŒ–

| æ–¹æ³• | æ”¶æ•›é˜¶ | æ¯æ­¥è®¡ç®—é‡ | ä¼˜ç‚¹ | é€‚ç”¨ |
|------|--------|------------|------|------|
| æœ€é€Ÿä¸‹é™ | çº¿æ€§ | $O(n)$ | ç®€å• | è¿œç¦»æœ€ä¼˜è§£æ—¶ |
| å…±è½­æ¢¯åº¦ | è¶…çº¿æ€§ | $O(n)$ | ä¸éœ€Hessian | å¤§è§„æ¨¡é—®é¢˜ |
| ç‰›é¡¿æ³• | äºŒæ¬¡ | $O(n^3)$ | æ”¶æ•›å¿« | å°è§„æ¨¡ã€è¿‘æœ€ä¼˜è§£ |
| BFGS | è¶…çº¿æ€§ | $O(n^2)$ | å¹³è¡¡æ•ˆç‡ä¸é€Ÿåº¦ | ä¸­ç­‰è§„æ¨¡ |

### çº¦æŸä¼˜åŒ–

| æ–¹æ³• | ç±»å‹ | ä¼˜ç‚¹ | ç¼ºç‚¹ |
|------|------|------|------|
| KKTæ¡ä»¶ | ç†è®º | æœ€ä¼˜æ€§åˆ¤æ–­ | éš¾ç›´æ¥æ±‚è§£ |
| ç½šå‡½æ•°æ³• | æ•°å€¼ | ç®€å•ã€é€šç”¨ | ç—…æ€ã€å‚æ•°æ•æ„Ÿ |
| å•çº¯å½¢æ³• | çº¿æ€§è§„åˆ’ | ç²¾ç¡®ã€é«˜æ•ˆ | ä»…é™çº¿æ€§ |

### å…¨å±€ä¼˜åŒ–

| æ–¹æ³• | ç‰¹ç‚¹ | åº”ç”¨ |
|------|------|------|
| é—ä¼ ç®—æ³• | ç¾¤ä½“æœç´¢ | å¤šå³°å‡½æ•° |
| æ¨¡æ‹Ÿé€€ç« | æ¦‚ç‡æ¥å— | ç»„åˆä¼˜åŒ– |

---

**æœ¬ç« é‡ç‚¹**ï¼š
- æœ€é€Ÿä¸‹é™æ³•ä¸å…±è½­æ¢¯åº¦æ³•
- ç‰›é¡¿æ³•ä¸BFGSæ‹Ÿç‰›é¡¿æ³•
- KKTæ¡ä»¶ä¸ç½šå‡½æ•°æ³•
- å•çº¯å½¢æ³•åŸºæœ¬æ€æƒ³
- å…¨å±€ä¼˜åŒ–æ–¹æ³•

**å…¨ä¹¦å®Œæˆï¼** ğŸ‰
