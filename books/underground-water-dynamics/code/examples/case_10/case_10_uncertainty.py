"""
æ¡ˆä¾‹10ï¼šä¸ç¡®å®šæ€§é‡åŒ–

æ¼”ç¤ºä¸ç¡®å®šæ€§é‡åŒ–çš„å„ç§æ–¹æ³•ï¼š
1. Monte Carloä¸ç¡®å®šæ€§åˆ†æž
2. GLUEæ–¹æ³•
3. Bootstrapæ–¹æ³•
4. ä¸ç¡®å®šæ€§åˆ†è§£
5. æ–¹æ³•å¯¹æ¯”
"""

import numpy as np
import matplotlib.pyplot as plt
from matplotlib import rcParams
import sys
import os

sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '../../..')))

from gwflow.solvers.steady_state import solve_1d_steady_gw
from gwflow.calibration.uncertainty import (
    monte_carlo_uncertainty,
    glue_analysis,
    bootstrap_uncertainty,
    propagate_uncertainty
)
from gwflow.calibration.optimization import levenberg_marquardt

# è®¾ç½®ä¸­æ–‡å­—ä½“
rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
rcParams['axes.unicode_minus'] = False

print("=" * 70)
print("æ¡ˆä¾‹10ï¼šä¸ç¡®å®šæ€§é‡åŒ–")
print("=" * 70)


# ============================================================================
# ç¬¬1æ­¥ï¼šç”Ÿæˆæ•°æ®
# ============================================================================

def forward_model_vector(params: np.ndarray) -> np.ndarray:
    """ä¸€ç»´ç¨³æ€åœ°ä¸‹æ°´æµåŠ¨ï¼ˆè¿”å›žè§‚æµ‹ç‚¹æ°´å¤´ï¼‰"""
    K, h_left, h_right = params
    
    L = 1000.0
    nx = 100
    
    h = solve_1d_steady_gw(K=K, L=L, h0=h_left, hL=h_right, nx=nx)
    
    # 10ä¸ªè§‚æµ‹ç‚¹
    obs_indices = np.linspace(10, 90, 10, dtype=int)
    return h[obs_indices]


print("\n1. ç”Ÿæˆè§‚æµ‹æ•°æ®")
print("-" * 70)

# çœŸå®žå‚æ•°
K_true = 10.0
h_left_true = 20.0
h_right_true = 10.0
params_true = np.array([K_true, h_left_true, h_right_true])

print(f"çœŸå®žå‚æ•°:")
print(f"  K = {K_true:.2f} m/day")
print(f"  h_left = {h_left_true:.2f} m")
print(f"  h_right = {h_right_true:.2f} m")

# ç”Ÿæˆè§‚æµ‹æ•°æ®
np.random.seed(42)
h_true = forward_model_vector(params_true)
sigma_obs = 0.15  # è§‚æµ‹è¯¯å·®
noise = np.random.normal(0, sigma_obs, len(h_true))
h_observed = h_true + noise

print(f"\nè§‚æµ‹ç‚¹æ•°é‡: {len(h_observed)}")
print(f"è§‚æµ‹è¯¯å·®æ ‡å‡†å·®: {sigma_obs:.2f} m")


# ============================================================================
# ç¬¬2æ­¥ï¼šå‚æ•°çŽ‡å®šï¼ˆèŽ·å¾—æ ¡å‡†å‚æ•°ï¼‰
# ============================================================================

print("\n" + "=" * 70)
print("2. å‚æ•°çŽ‡å®š")
print("=" * 70)

initial_params = np.array([15.0, 22.0, 12.0])
bounds = [(5.0, 20.0), (15.0, 25.0), (8.0, 15.0)]

result_calib = levenberg_marquardt(
    forward_model=forward_model_vector,
    initial_params=initial_params,
    observed=h_observed,
    bounds=bounds,
    verbose=False
)

params_calibrated = result_calib['parameters']

print(f"çŽ‡å®šç»“æžœ:")
print(f"  K = {params_calibrated[0]:.4f} m/day")
print(f"  h_left = {params_calibrated[1]:.4f} m")
print(f"  h_right = {params_calibrated[2]:.4f} m")


# ============================================================================
# ç¬¬3æ­¥ï¼šMonte Carloä¸ç¡®å®šæ€§åˆ†æž
# ============================================================================

print("\n" + "=" * 70)
print("3. Monte Carloä¸ç¡®å®šæ€§åˆ†æž")
print("=" * 70)

# å®šä¹‰å‚æ•°ä¸ç¡®å®šæ€§ï¼ˆåŸºäºŽçŽ‡å®šç»“æžœçš„å‡è®¾ï¼‰
param_distributions = [
    ('normal', (params_calibrated[0], 0.5)),    # K
    ('normal', (params_calibrated[1], 0.3)),    # h_left
    ('normal', (params_calibrated[2], 0.3))     # h_right
]

mc_result = monte_carlo_uncertainty(
    forward_model=forward_model_vector,
    param_distributions=param_distributions,
    n_samples=2000,
    verbose=True
)

print(f"\nè¾“å‡ºä¸ç¡®å®šæ€§:")
print(f"  å‡å€¼æ ‡å‡†å·®: {np.mean(mc_result['output_std']):.4f} m")


# ============================================================================
# ç¬¬4æ­¥ï¼šGLUEåˆ†æž
# ============================================================================

print("\n" + "=" * 70)
print("4. GLUEåˆ†æž")
print("=" * 70)

glue_result = glue_analysis(
    forward_model=forward_model_vector,
    observations=h_observed,
    param_bounds=bounds,
    n_samples=5000,
    likelihood_threshold=0.7,
    verbose=True
)


# ============================================================================
# ç¬¬5æ­¥ï¼šBootstrapä¸ç¡®å®šæ€§
# ============================================================================

print("\n" + "=" * 70)
print("5. Bootstrapä¸ç¡®å®šæ€§åˆ†æž")
print("=" * 70)

bootstrap_result = bootstrap_uncertainty(
    forward_model=forward_model_vector,
    observations=h_observed,
    calibrated_params=params_calibrated,
    n_bootstrap=1000,
    noise_std=sigma_obs,
    verbose=True
)


# ============================================================================
# ç¬¬6æ­¥ï¼šå¯è§†åŒ–
# ============================================================================

print("\n" + "=" * 70)
print("6. ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨")
print("=" * 70)

x_obs = np.arange(len(h_observed))

# å›¾1ï¼šMonte Carloä¸ç¡®å®šæ€§
fig, ax = plt.subplots(figsize=(10, 6))

# çœŸå®žå€¼
ax.plot(x_obs, h_true, 'g--', linewidth=2, label='çœŸå®žå€¼', zorder=4)

# è§‚æµ‹å€¼
ax.scatter(x_obs, h_observed, color='black', s=100, zorder=5,
          label='è§‚æµ‹å€¼', marker='o')

# Monte Carloé¢„æµ‹å‡å€¼
ax.plot(x_obs, mc_result['output_mean'], 'b-', linewidth=2,
       label='Monte Carloå‡å€¼', zorder=3)

# 95%ç½®ä¿¡åŒºé—´
ax.fill_between(x_obs, mc_result['output_ci_lower'], mc_result['output_ci_upper'],
                alpha=0.3, color='blue', label='95% Monte CarloåŒºé—´')

# çŽ‡å®šå€¼
h_calibrated = forward_model_vector(params_calibrated)
ax.plot(x_obs, h_calibrated, 'r-.', linewidth=2,
       label='çŽ‡å®šå€¼', zorder=3)

ax.set_xlabel('è§‚æµ‹ç‚¹ç´¢å¼•')
ax.set_ylabel('æ°´å¤´ (m)')
ax.set_title('Monte Carloä¸ç¡®å®šæ€§åˆ†æž')
ax.legend()
ax.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('case_10_monte_carlo.png', dpi=300, bbox_inches='tight')
print("å·²ä¿å­˜: case_10_monte_carlo.png")
plt.close()

# å›¾2ï¼šGLUEä¸ç¡®å®šæ€§
fig, ax = plt.subplots(figsize=(10, 6))

# è§‚æµ‹å€¼
ax.scatter(x_obs, h_observed, color='black', s=100, zorder=5,
          label='è§‚æµ‹å€¼', marker='o')

# çœŸå®žå€¼
ax.plot(x_obs, h_true, 'g--', linewidth=2, label='çœŸå®žå€¼', zorder=4)

# GLUEé¢„æµ‹ï¼ˆç»˜åˆ¶è¡Œä¸ºå‚æ•°é›†çš„æ¨¡æ‹Ÿï¼‰
for i in range(min(100, glue_result['n_behavioral'])):
    ax.plot(x_obs, glue_result['behavioral_simulations'][i],
           'gray', alpha=0.1, linewidth=0.5)

# GLUEåŠ æƒå‡å€¼
ax.plot(x_obs, glue_result['weighted_mean'], 'r-', linewidth=2,
       label='GLUEåŠ æƒå‡å€¼', zorder=3)

# 95%ç½®ä¿¡åŒºé—´
ax.fill_between(x_obs, glue_result['ci_lower'], glue_result['ci_upper'],
                alpha=0.3, color='red', label='95% GLUEåŒºé—´')

ax.set_xlabel('è§‚æµ‹ç‚¹ç´¢å¼•')
ax.set_ylabel('æ°´å¤´ (m)')
ax.set_title(f'GLUEåˆ†æž (è¡Œä¸ºå‚æ•°é›†: {glue_result["n_behavioral"]}/{glue_result["n_total"]})')
ax.legend()
ax.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('case_10_glue.png', dpi=300, bbox_inches='tight')
print("å·²ä¿å­˜: case_10_glue.png")
plt.close()

# å›¾3ï¼šæ–¹æ³•å¯¹æ¯”
fig, ax = plt.subplots(figsize=(10, 6))

# è§‚æµ‹å€¼
ax.scatter(x_obs, h_observed, color='black', s=100, zorder=5,
          label='è§‚æµ‹å€¼', marker='o')

# çœŸå®žå€¼
ax.plot(x_obs, h_true, 'g--', linewidth=3, label='çœŸå®žå€¼', zorder=4)

# Monte Carlo
ax.plot(x_obs, mc_result['output_mean'], 'b-', linewidth=2,
       label='Monte Carlo', alpha=0.7)
ax.fill_between(x_obs, mc_result['output_ci_lower'], mc_result['output_ci_upper'],
                alpha=0.2, color='blue')

# GLUE
ax.plot(x_obs, glue_result['weighted_mean'], 'r-', linewidth=2,
       label='GLUE', alpha=0.7)
ax.fill_between(x_obs, glue_result['ci_lower'], glue_result['ci_upper'],
                alpha=0.2, color='red')

# Bootstrap
ax.plot(x_obs, bootstrap_result['output_mean'], 'm-.', linewidth=2,
       label='Bootstrap', alpha=0.7)
ax.fill_between(x_obs, bootstrap_result['ci_lower'], bootstrap_result['ci_upper'],
                alpha=0.2, color='magenta')

ax.set_xlabel('è§‚æµ‹ç‚¹ç´¢å¼•')
ax.set_ylabel('æ°´å¤´ (m)')
ax.set_title('ä¸ç¡®å®šæ€§é‡åŒ–æ–¹æ³•å¯¹æ¯”')
ax.legend()
ax.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('case_10_methods_comparison.png', dpi=300, bbox_inches='tight')
print("å·²ä¿å­˜: case_10_methods_comparison.png")
plt.close()

# å›¾4ï¼šè¡Œä¸ºå‚æ•°é›†åˆ†å¸ƒï¼ˆGLUEï¼‰
fig, axes = plt.subplots(1, 3, figsize=(15, 4))

param_names = ['K (m/day)', 'h_left (m)', 'h_right (m)']
true_values = params_true

for i, (ax, name, true_val) in enumerate(zip(axes, param_names, true_values)):
    # æ‰€æœ‰æ ·æœ¬çš„ç›´æ–¹å›¾ï¼ˆæµ…è‰²ï¼‰
    ax.hist(glue_result['behavioral_params'][:, i], bins=30, alpha=0.5,
           color='lightgray', edgecolor='black', density=True,
           label='è¡Œä¸ºå‚æ•°é›†')
    
    # çœŸå€¼
    ax.axvline(true_val, color='green', linestyle=':', linewidth=3,
              label='çœŸå€¼')
    
    # çŽ‡å®šå€¼
    ax.axvline(params_calibrated[i], color='red', linestyle='--',
              linewidth=2, label='çŽ‡å®šå€¼')
    
    ax.set_xlabel(name)
    ax.set_ylabel('æ¦‚çŽ‡å¯†åº¦')
    ax.legend()
    ax.grid(True, alpha=0.3)

axes[0].set_title('GLUEè¡Œä¸ºå‚æ•°é›†åˆ†å¸ƒ')
plt.tight_layout()
plt.savefig('case_10_behavioral_params.png', dpi=300, bbox_inches='tight')
print("å·²ä¿å­˜: case_10_behavioral_params.png")
plt.close()

# å›¾5ï¼šä¸ç¡®å®šæ€§åŒºé—´å®½åº¦å¯¹æ¯”
fig, ax = plt.subplots(figsize=(10, 6))

mc_width = mc_result['output_ci_upper'] - mc_result['output_ci_lower']
glue_width = glue_result['ci_upper'] - glue_result['ci_lower']
bootstrap_width = bootstrap_result['ci_upper'] - bootstrap_result['ci_lower']

width_data = 0.25
positions = np.arange(len(x_obs))

ax.bar(positions - width_data, mc_width, width_data,
      label='Monte Carlo', color='steelblue', edgecolor='black')
ax.bar(positions, glue_width, width_data,
      label='GLUE', color='coral', edgecolor='black')
ax.bar(positions + width_data, bootstrap_width, width_data,
      label='Bootstrap', color='lightgreen', edgecolor='black')

ax.set_xlabel('è§‚æµ‹ç‚¹ç´¢å¼•')
ax.set_ylabel('95% CIå®½åº¦ (m)')
ax.set_title('ä¸ç¡®å®šæ€§åŒºé—´å®½åº¦å¯¹æ¯”')
ax.set_xticks(positions)
ax.set_xticklabels(x_obs)
ax.legend()
ax.grid(True, alpha=0.3, axis='y')

plt.tight_layout()
plt.savefig('case_10_uncertainty_width.png', dpi=300, bbox_inches='tight')
print("å·²ä¿å­˜: case_10_uncertainty_width.png")
plt.close()


# ============================================================================
# ç¬¬7æ­¥ï¼šæ–¹æ³•æ€»ç»“
# ============================================================================

print("\n" + "=" * 70)
print("7. æ–¹æ³•æ€»ç»“ä¸Žå¯¹æ¯”")
print("=" * 70)

print("\nMonte Carloæ–¹æ³•:")
print("  âœ… å‚æ•°ä¸ç¡®å®šæ€§ä¼ æ’­")
print("  âœ… é€‚åˆå·²çŸ¥å‚æ•°åˆ†å¸ƒ")
print("  âœ… ç†è®ºæ¸…æ™°")
print(f"  è®¡ç®—æˆæœ¬: {mc_result['n_samples']} æ¬¡æ¨¡åž‹è¿è¡Œ")

print("\nGLUEæ–¹æ³•:")
print("  âœ… éžæ­£å¼è´å¶æ–¯")
print("  âœ… è¯†åˆ«è¡Œä¸ºå‚æ•°é›†")
print("  âœ… é€‚åˆæ¨¡åž‹ç»“æž„ä¸ç¡®å®šæ€§")
print(f"  è®¡ç®—æˆæœ¬: {glue_result['n_total']} æ¬¡æ¨¡åž‹è¿è¡Œ")
print(f"  æŽ¥å—çŽ‡: {glue_result['acceptance_rate']*100:.1f}%")

print("\nBootstrapæ–¹æ³•:")
print("  âœ… æ®‹å·®é‡é‡‡æ ·")
print("  âœ… è§‚æµ‹è¯¯å·®é‡åŒ–")
print("  âœ… ä¸éœ€è¦å‚æ•°åˆ†å¸ƒå‡è®¾")
print(f"  è®¡ç®—æˆæœ¬: {bootstrap_result['n_bootstrap']} æ¬¡ï¼ˆç®€åŒ–ç‰ˆï¼‰")

print("\nå¹³å‡ä¸ç¡®å®šæ€§ï¼ˆæ ‡å‡†å·®ï¼‰:")
print(f"  Monte Carlo: {np.mean(mc_result['output_std']):.4f} m")
print(f"  GLUE: {np.mean(glue_result['ci_upper'] - glue_result['weighted_mean']):.4f} m")
print(f"  Bootstrap: {np.mean(bootstrap_result['output_std']):.4f} m")

print("\nè¦†ç›–çŽ‡åˆ†æžï¼ˆçœŸå€¼è¢«ç½®ä¿¡åŒºé—´è¦†ç›–çš„æ¯”ä¾‹ï¼‰:")
mc_coverage = np.mean((h_true >= mc_result['output_ci_lower']) & 
                      (h_true <= mc_result['output_ci_upper']))
glue_coverage = np.mean((h_true >= glue_result['ci_lower']) & 
                        (h_true <= glue_result['ci_upper']))
bootstrap_coverage = np.mean((h_true >= bootstrap_result['ci_lower']) & 
                             (h_true <= bootstrap_result['ci_upper']))

print(f"  Monte Carlo: {mc_coverage*100:.1f}%")
print(f"  GLUE: {glue_coverage*100:.1f}%")
print(f"  Bootstrap: {bootstrap_coverage*100:.1f}%")

print("\n" + "=" * 70)
print("æ¡ˆä¾‹10å®Œæˆï¼")
print("=" * 70)
print("\nè¾“å‡ºæ–‡ä»¶:")
print("  1. case_10_monte_carlo.png - Monte Carloä¸ç¡®å®šæ€§")
print("  2. case_10_glue.png - GLUEåˆ†æž")
print("  3. case_10_methods_comparison.png - æ–¹æ³•å¯¹æ¯”")
print("  4. case_10_behavioral_params.png - è¡Œä¸ºå‚æ•°é›†åˆ†å¸ƒ")
print("  5. case_10_uncertainty_width.png - ä¸ç¡®å®šæ€§åŒºé—´å®½åº¦")

print("\n" + "=" * 70)
print("ðŸŽ‰ å‚æ•°çŽ‡å®šç¯‡å…¨éƒ¨å®Œæˆï¼")
print("=" * 70)
